import requests
from bs4 import BeautifulSoup
import logging

# --- C·∫•u h√¨nh Scraping API (ƒê√£ ho√†n l·∫°i theo y√™u c·∫ßu) ---
# Gi·ªØ nguy√™n logic key nh∆∞ phi√™n b·∫£n g·ªëc c·ªßa b·∫°n.
SCRAPING_API_KEYS = [
    {"name": "ScraperAPI", "key": "c44d4b74755d8d58d5b4ff9712efe0b0"},
    {"name": "ScrapingBee", "key": "RCZMD6OZA5X5LD4WJA5Z5AYD6Z6AW4TCUB82FJ98QE4A68N05UBFDHLRZIP2YLJ92JRUXEPC6EVW6QCG"},
]
current_scraping_key_index = 0

# --- H√†m l·∫•y d·ªØ li·ªáu ch∆∞∆°ng ---

def get_chapter_data(base_url: str, chapter_number: int) -> tuple[str, str]:
    """
    L·∫•y d·ªØ li·ªáu ch∆∞∆°ng b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt lo·∫°t c√°c API scraping ƒë·ªÉ tƒÉng ƒë·ªô tin c·∫≠y.
    H√†m s·∫Ω t·ª± ƒë·ªông xoay v√≤ng key n·∫øu g·∫∑p l·ªói v√† c√≥ logic t√¨m ki·∫øm n·ªôi dung linh ho·∫°t h∆°n.
    """
    global current_scraping_key_index
    
    # S·ª¨A L·ªñI: Lo·∫°i b·ªè d·∫•u g·∫°ch ch√©o (/) th·ª´a ·ªü cu·ªëi base_url ƒë·ªÉ tr√°nh l·ªói URL k√©p
    target_url = f"{base_url.rstrip('/')}/chapter-{chapter_number}"
    
    # C√°c t·ª´ kh√≥a r√°c c·∫ßn lo·∫°i b·ªè kh·ªèi n·ªôi dung
    junk_keywords = [
        'nov‚ÑØls', 'follow current', 'novelupdates', 'wuxiaworld', 'read the latest chapter', 
        'f(r)eewebnovùíÜl', 'this chaptùôör is updated by fr(e)ewùíÜbnov(e)l.com', 
        'this chaptùôör', 'fr(e)ewùíÜbnov(e)l.com', 'is updated by'
    ]

    # Th·ª≠ t·∫•t c·∫£ c√°c API key c√≥ trong danh s√°ch
    if not SCRAPING_API_KEYS:
        logging.error("Danh s√°ch SCRAPING_API_KEYS r·ªóng. Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu.")
        return None, None

    for i in range(len(SCRAPING_API_KEYS)):
        current_service = SCRAPING_API_KEYS[current_scraping_key_index]
        api_key = current_service["key"]
        service_name = current_service["name"]
        
        logging.info(f"API Scraping: ƒêang th·ª≠ l·∫•y ch∆∞∆°ng {chapter_number} b·∫±ng {service_name}...")
        
        # C·∫•u h√¨nh params cho t·ª´ng d·ªãch v·ª•
        if service_name == "ScraperAPI":
            params = {"api_key": api_key, "url": target_url}
            api_url = "https://api.scraperapi.com/"
        elif service_name == "ScrapingBee":
            params = {"api_key": api_key, "url": target_url, "render_js": "false"}
            api_url = "https://app.scrapingbee.com/api/v1/"
        else:
            logging.warning(f"D·ªãch v·ª• scraping '{service_name}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. B·ªè qua.")
            current_scraping_key_index = (current_scraping_key_index + 1) % len(SCRAPING_API_KEYS)
            continue

        try:
            response = requests.get(api_url, params=params, timeout=120)
            
            # X·ª≠ l√Ω l·ªói v√† xoay v√≤ng key
            if response.status_code in [401, 403, 429]: # Unauthorized, Forbidden, Too Many Requests
                logging.warning(f"D·ªãch v·ª• {service_name} b√°o l·ªói {response.status_code}. ƒêang chuy·ªÉn key...")
                current_scraping_key_index = (current_scraping_key_index + 1) % len(SCRAPING_API_KEYS)
                continue
            
            response.raise_for_status() # N√©m l·ªói cho c√°c status code 4xx/5xx kh√°c
            html_content = response.text
            
            # Ph√¢n t√≠ch HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # --- LOGIC L·∫§Y TI√äU ƒê·ªÄ V√Ä N·ªòI DUNG ƒê∆Ø·ª¢C C·∫¢I TI·∫æN ---

            # 1. T√¨m v√πng ch·ª©a n·ªôi dung ch√≠nh
            content_selectors = ['div.txt', 'div#article', 'div#content', 'div.reading-content', 'div.chapter-content']
            content_element = None
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    logging.info(f"T√¨m th·∫•y v√πng ch·ª©a n·ªôi dung b·∫±ng selector: '{selector}'")
                    break
            
            if not content_element:
                raise ValueError(f"Kh√¥ng t√¨m th·∫•y th·∫ª div ch·ª©a n·ªôi dung ch√≠nh v·ªõi c√°c selector ƒë√£ th·ª≠: {content_selectors}")

            # 2. T√¨m ti√™u ƒë·ªÅ (logic ƒë∆∞·ª£c c·∫≠p nh·∫≠t)
            # ∆Øu ti√™n t√¨m c√°c selector c·ª• th·ªÉ ·ªü b·∫•t c·ª© ƒë√¢u trong trang tr∆∞·ªõc
            title_selectors_specific = ['span.chapter', 'h3.tit']
            # Sau ƒë√≥ t√¨m c√°c selector chung chung b√™n trong v√πng n·ªôi dung
            title_selectors_generic = ['h1', 'h2', 'h3', 'h4', '.chapter-title', '.title']
            
            title_element = None
            
            # Th·ª≠ selector c·ª• th·ªÉ tr∆∞·ªõc tr√™n to√†n b·ªô `soup`
            for selector in title_selectors_specific:
                title_element = soup.select_one(selector)
                if title_element:
                    logging.info(f"T√¨m th·∫•y ti√™u ƒë·ªÅ b·∫±ng selector c·ª• th·ªÉ: '{selector}'")
                    break
            
            # N·∫øu kh√¥ng th·∫•y, th·ª≠ selector chung chung b√™n trong `content_element`
            if not title_element:
                for selector in title_selectors_generic:
                    title_element = content_element.select_one(selector)
                    if title_element:
                        logging.info(f"T√¨m th·∫•y ti√™u ƒë·ªÅ b·∫±ng selector chung: '{selector}'")
                        break

            title = title_element.get_text(strip=True) if title_element else f"Ch∆∞∆°ng {chapter_number}"
            
            # N·∫øu t√¨m th·∫•y th·∫ª ti√™u ƒë·ªÅ, x√≥a n√≥ ƒëi ƒë·ªÉ kh√¥ng b·ªã l·∫∑p l·∫°i trong n·ªôi dung
            # ƒêi·ªÅu n√†y an to√†n ngay c·∫£ khi n√≥ n·∫±m ngo√†i content_element
            if title_element:
                 title_element.decompose()

            # 3. L·ªçc v√† l√†m s·∫°ch n·ªôi dung
            story_lines = []
            for p in content_element.find_all('p'):
                text = p.get_text(strip=True)
                if text and not any(keyword in text.lower() for keyword in junk_keywords):
                    story_lines.append(text)
            
            story_text = '\n\n'.join(story_lines)

            if not story_text:
                raise ValueError("Kh√¥ng t√¨m th·∫•y n·ªôi dung truy·ªán (th·∫ª <p>) trong HTML sau khi l·ªçc.")
            
            logging.info(f"L·∫•y th√†nh c√¥ng ch∆∞∆°ng {chapter_number} b·∫±ng {service_name}.")
            return title, story_text

        except requests.exceptions.RequestException as e:
            logging.error(f"L·ªói k·∫øt n·ªëi v·ªõi {service_name}: {e}. ƒêang th·ª≠ key ti·∫øp theo...")
            current_scraping_key_index = (current_scraping_key_index + 1) % len(SCRAPING_API_KEYS)
            continue
        except ValueError as e:
            logging.error(f"L·ªói ph√¢n t√≠ch HTML: {e}. C√≥ th·ªÉ c·∫•u tr√∫c trang web ƒë√£ thay ƒë·ªïi.")
            current_scraping_key_index = (current_scraping_key_index + 1) % len(SCRAPING_API_KEYS)
            continue

    # N·∫øu ƒë√£ th·ª≠ h·∫øt c√°c key m√† v·∫´n l·ªói
    logging.error(f"ƒê√£ th·ª≠ h·∫øt t·∫•t c·∫£ API scraping nh∆∞ng kh√¥ng l·∫•y ƒë∆∞·ª£c ch∆∞∆°ng {chapter_number} t·ª´ URL: {target_url}")
    return None, None
